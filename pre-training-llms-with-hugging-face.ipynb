{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install transformers==4.40.0 \n!pip install -U git+https://github.com/huggingface/transformers\n!pip install datasets # 2.15.0\n!pip install portalocker>=2/0.0\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install torch==2.3.0\n!pip install -U torchvision\n!pip install protobuf==3.20.*","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-21T06:27:52.398229Z","iopub.execute_input":"2024-10-21T06:27:52.398535Z","iopub.status.idle":"2024-10-21T06:35:49.984138Z","shell.execute_reply.started":"2024-10-21T06:27:52.398502Z","shell.execute_reply":"2024-10-21T06:35:49.983212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets==2.15.0","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:37:23.513805Z","iopub.execute_input":"2024-10-21T06:37:23.514197Z","iopub.status.idle":"2024-10-21T06:38:32.011842Z","shell.execute_reply.started":"2024-10-21T06:37:23.514164Z","shell.execute_reply":"2024-10-21T06:38:32.010756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoConfig,AutoModelForCausalLM,AutoModelForSequenceClassification,BertConfig,BertForMaskedLM,TrainingArguments, Trainer, TrainingArguments\nfrom transformers import AutoTokenizer,BertTokenizerFast,TextDataset,DataCollatorForLanguageModeling\nfrom transformers import pipeline\nfrom datasets import load_dataset\n\nfrom tqdm.auto import tqdm\nimport math\nimport time\nimport os\n\n\n# You can also use this section to suppress warnings generated by your code:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:42:38.253191Z","iopub.execute_input":"2024-10-21T06:42:38.253890Z","iopub.status.idle":"2024-10-21T06:42:38.260722Z","shell.execute_reply.started":"2024-10-21T06:42:38.253850Z","shell.execute_reply":"2024-10-21T06:42:38.259661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the environment variable TOKENIZERS_PARALLELISM to 'false'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:42:45.465512Z","iopub.execute_input":"2024-10-21T06:42:45.465911Z","iopub.status.idle":"2024-10-21T06:42:45.470176Z","shell.execute_reply.started":"2024-10-21T06:42:45.465872Z","shell.execute_reply":"2024-10-21T06:42:45.469145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\npipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer, device=0)\nprint(pipe(\"This movie was really\")[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:04.875863Z","iopub.execute_input":"2024-10-21T06:44:04.876265Z","iopub.status.idle":"2024-10-21T06:44:07.312434Z","shell.execute_reply.started":"2024-10-21T06:44:04.876226Z","shell.execute_reply":"2024-10-21T06:44:07.311479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the datasets\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:13.759699Z","iopub.execute_input":"2024-10-21T06:44:13.760658Z","iopub.status.idle":"2024-10-21T06:44:17.123934Z","shell.execute_reply.started":"2024-10-21T06:44:13.760614Z","shell.execute_reply":"2024-10-21T06:44:17.122993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:20.362585Z","iopub.execute_input":"2024-10-21T06:44:20.363452Z","iopub.status.idle":"2024-10-21T06:44:20.368255Z","shell.execute_reply.started":"2024-10-21T06:44:20.363410Z","shell.execute_reply":"2024-10-21T06:44:20.367306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check a sample record\ndataset[\"train\"][400]","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:24.501702Z","iopub.execute_input":"2024-10-21T06:44:24.502076Z","iopub.status.idle":"2024-10-21T06:44:24.512192Z","shell.execute_reply.started":"2024-10-21T06:44:24.502042Z","shell.execute_reply":"2024-10-21T06:44:24.511225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path to save the datasets to text files\noutput_file_train = \"wikitext_dataset_train.txt\"\noutput_file_test = \"wikitext_dataset_test.txt\"\n\n# Open the output file in write mode\nwith open(output_file_train, \"w\", encoding=\"utf-8\") as f:\n    # Iterate over each example in the dataset\n    for example in dataset[\"train\"]:\n        # Write the example text to the file\n        f.write(example[\"text\"] + \"\\n\")\n\n# Open the output file in write mode\nwith open(output_file_test, \"w\", encoding=\"utf-8\") as f:\n    # Iterate over each example in the dataset\n    for example in dataset[\"test\"]:\n        # Write the example text to the file\n        f.write(example[\"text\"] + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:28.050715Z","iopub.execute_input":"2024-10-21T06:44:28.051679Z","iopub.status.idle":"2024-10-21T06:44:29.083739Z","shell.execute_reply.started":"2024-10-21T06:44:28.051635Z","shell.execute_reply":"2024-10-21T06:44:29.082735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a tokenizer from existing one to re-use special tokens\nbert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:32.521204Z","iopub.execute_input":"2024-10-21T06:44:32.521615Z","iopub.status.idle":"2024-10-21T06:44:33.225837Z","shell.execute_reply.started":"2024-10-21T06:44:32.521577Z","shell.execute_reply":"2024-10-21T06:44:33.225122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert-base-uncased'\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:38.270462Z","iopub.execute_input":"2024-10-21T06:44:38.271179Z","iopub.status.idle":"2024-10-21T06:44:40.788251Z","shell.execute_reply.started":"2024-10-21T06:44:38.271141Z","shell.execute_reply":"2024-10-21T06:44:40.787289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the BERT configuration\nconfig = BertConfig(\n    vocab_size=len(bert_tokenizer.get_vocab()),  # Specify the vocabulary size(Make sure this number equals the vocab_size of the tokenizer)\n    hidden_size=768,  # Set the hidden size\n    num_hidden_layers=12,  # Set the number of layers\n    num_attention_heads=12,  # Set the number of attention heads\n    intermediate_size=3072,  # Set the intermediate size\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:44.866024Z","iopub.execute_input":"2024-10-21T06:44:44.866407Z","iopub.status.idle":"2024-10-21T06:44:44.883822Z","shell.execute_reply.started":"2024-10-21T06:44:44.866370Z","shell.execute_reply":"2024-10-21T06:44:44.882817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the BERT model for pre-training\nmodel = BertForMaskedLM(config)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:48.184925Z","iopub.execute_input":"2024-10-21T06:44:48.185319Z","iopub.status.idle":"2024-10-21T06:44:50.259196Z","shell.execute_reply.started":"2024-10-21T06:44:48.185282Z","shell.execute_reply":"2024-10-21T06:44:50.258331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check model configuration\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:55.250451Z","iopub.execute_input":"2024-10-21T06:44:55.250801Z","iopub.status.idle":"2024-10-21T06:44:55.259143Z","shell.execute_reply.started":"2024-10-21T06:44:55.250768Z","shell.execute_reply":"2024-10-21T06:44:55.258144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the pre-training data as a TextDataset\ntrain_dataset = TextDataset(\n    tokenizer=bert_tokenizer,\n    file_path=\"wikitext_dataset_train.txt\",  # Path to your pre-training data file\n    block_size=128  # Set the desired block size for training\n)\ntest_dataset = TextDataset(\n    tokenizer=bert_tokenizer,\n    file_path=\"wikitext_dataset_test.txt\",  # Path to your pre-training data file\n    block_size=128  # Set the desired block size for training\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:44:58.864726Z","iopub.execute_input":"2024-10-21T06:44:58.865502Z","iopub.status.idle":"2024-10-21T06:45:14.407055Z","shell.execute_reply.started":"2024-10-21T06:44:58.865459Z","shell.execute_reply":"2024-10-21T06:45:14.406030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:45:21.240673Z","iopub.execute_input":"2024-10-21T06:45:21.241068Z","iopub.status.idle":"2024-10-21T06:45:21.250734Z","shell.execute_reply.started":"2024-10-21T06:45:21.241029Z","shell.execute_reply":"2024-10-21T06:45:21.249773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the data collator for language modeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:45:24.973039Z","iopub.execute_input":"2024-10-21T06:45:24.973816Z","iopub.status.idle":"2024-10-21T06:45:24.978045Z","shell.execute_reply.started":"2024-10-21T06:45:24.973774Z","shell.execute_reply":"2024-10-21T06:45:24.977086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check how collator transforms a sample input data record\ndata_collator([train_dataset[0]])","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:45:28.160897Z","iopub.execute_input":"2024-10-21T06:45:28.161779Z","iopub.status.idle":"2024-10-21T06:45:28.178943Z","shell.execute_reply.started":"2024-10-21T06:45:28.161739Z","shell.execute_reply":"2024-10-21T06:45:28.178050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./trained_model\",  # Specify the output directory for the trained model\n    overwrite_output_dir=True,\n    do_eval=True,\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    num_train_epochs=10,  # Specify the number of training epochs\n    per_device_train_batch_size=2,  # Set the batch size for training\n    save_total_limit=2,  # Limit the total number of saved checkpoints\n    logging_steps = 20,\n    fp16=True if torch.cuda.is_available() else False\n    \n)\n\n# Instantiate the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n# Start the pre-training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:48:07.431647Z","iopub.execute_input":"2024-10-21T06:48:07.432065Z","iopub.status.idle":"2024-10-21T09:13:15.666538Z","shell.execute_reply.started":"2024-10-21T06:48:07.432025Z","shell.execute_reply":"2024-10-21T09:13:15.665630Z"},"trusted":true},"execution_count":null,"outputs":[]}]}